{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read line by line, remove any special characters but white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'104 Nash Hall Department of Fisheries and Wildlife Oregon State University Corvallis OR 973317501'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"\\\\+++|||\\n\\t104 Nash,,, Hall, Departm$$$ent of ###Fisheri##es and Wildlife; Oregon State University, Corvallis, OR 97331-7501.\"\n",
    "re.sub('[^A-Za-z0-9 ]+', '', test)\n",
    "# re.split(\", |; | \", t)\n",
    "# ''.join(e for e in t if e.isalnum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./final_orgs.txt\"\n",
    "samples = 1000\n",
    "data = []\n",
    "with open(filepath) as f:\n",
    "    while samples > 0:\n",
    "        data.append(re.sub('[^A-Za-z0-9 ]+', '', (next(f))))\n",
    "        samples -= 1\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Tf-Idf table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tf_idf(data)\n",
    "a.get_tfidf_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.39864781862538884, '10524 California Institute of Technology'),\n",
       " (0.3444626099831442,\n",
       "  '0ffice of Information Teclhnology Georgia Institute of TechnologyTAB'),\n",
       " (0.23503090133769758,\n",
       "  '10524 Robinson Lab California Institute of Technology Pasadena California 91125'),\n",
       " (0.09859764475914322,\n",
       "  '0601 class of Computer Science  Technology Hubei University of Technology'),\n",
       " (0.08551410564996555,\n",
       "  '100 Haiquan Road Dept of Food Science and Technology Shanghai Inst of Technology Shanghai China'),\n",
       " (0.08468069944196469,\n",
       "  '100 Bureau DR National Institute of Standards and Technology Gaithersburg MD 208998662 USA'),\n",
       " (0.07763169629565018,\n",
       "  '100 Bureau Drive Stop 8940 National Institute of Standards  Technology Gaithersburg Maryland 20899TAB'),\n",
       " (0.0773660503472803,\n",
       "  '100 Bureau Drive Stop 8423 National Institute of Standards and Technology Gaithersburg Maryland 208998423 USA'),\n",
       " (0.07322102476361969, '101st University Hospital Clinic Tbilisi Georgia'),\n",
       " (0.07061979351151958,\n",
       "  '100 Bureau Drive National Institute of Standards and Technology Gaithersburg MD 208998212 USA Gaithersburg Maryland 208998212 UNITED STATES')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# heapq.nlargest(10, a.query(\"University\"))\n",
    "heapq.nlargest(10, a.query(\"Georgia Institute of Technology\"))\n",
    "# heapq.nlargest(10, a.query(\"100 Radiation Center Oregon State University Corvallis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tf_idf(object):\n",
    "    def __init__(self, train_list):\n",
    "        self.train = train_list\n",
    "        self.train_num = len(train_list)\n",
    "        self.words_bag = set([word for sentence in train_list\n",
    "                              for word in sentence.lower().split(' ')])\n",
    "        for sentence in train_list:\n",
    "            two_gram = self._cal_two_gram(sentence)\n",
    "            self.words_bag = self.words_bag.union(two_gram)\n",
    "            three_gram = self._cal_three_gram(sentence)\n",
    "            self.words_bag = self.words_bag.union(three_gram)\n",
    "\n",
    "        self.words_count = {word: 0 for word in self.words_bag}\n",
    "        self.idfs = {}\n",
    "        self.tfs = []\n",
    "\n",
    "    def _cal_word_frequency(self):\n",
    "        for sentence in self.train:\n",
    "            for word in sentence.lower().split(' '):\n",
    "                self.words_count[word] += 1\n",
    "            for word_gram in self._cal_two_gram(sentence):\n",
    "                self.words_count[word_gram] += 1\n",
    "            for word_gram in self._cal_three_gram(sentence):\n",
    "                self.words_count[word_gram] += 1\n",
    "\n",
    "    def _cal_two_gram(self, sentence):\n",
    "        word_list = sentence.lower().split(' ')\n",
    "        two_gram_bag = set()\n",
    "        for i, _ in enumerate(word_list[:-1]):\n",
    "            two_gram = ' '.join([word_list[i], word_list[i+1]])\n",
    "            two_gram_bag.add(two_gram)\n",
    "        return two_gram_bag\n",
    "    \n",
    "    def _cal_three_gram(self, sentence):\n",
    "        word_list = sentence.lower().split(' ')\n",
    "        three_gram_bag = set()\n",
    "        for i, _ in enumerate(word_list[:-2]):\n",
    "            three_gram = ' '.join([word_list[i], word_list[i+1], word_list[i+2]])\n",
    "            three_gram_bag.add(three_gram)\n",
    "        return three_gram_bag\n",
    "\n",
    "\n",
    "    def _cal_tf(self):\n",
    "        for sentence in self.train:\n",
    "            word_list = sentence.lower().split(' ')\n",
    "            word_gram_list = list(self._cal_two_gram(sentence))\n",
    "            word_list += word_gram_list\n",
    "            word_gram_list = list(self._cal_three_gram(sentence))\n",
    "            word_list += word_gram_list\n",
    "            length = len(word_list)\n",
    "            word_tf = [1 / length**0.5] * length\n",
    "            self.tfs.append(dict(zip(word_list, word_tf)))\n",
    "\n",
    "    def _cal_idf(self):\n",
    "        self._cal_word_frequency()\n",
    "        for word in self.words_bag:\n",
    "            if self.words_count[word] == 0:\n",
    "                print(word)\n",
    "                print(self.words_count)\n",
    "            ratio = self.train_num / self.words_count[word]\n",
    "            self.idfs[word] = math.log10(ratio)\n",
    "\n",
    "    def get_tfidf_table(self):\n",
    "        self._cal_tf()\n",
    "        self._cal_idf()\n",
    "        self.tfidfs = self.tfs.copy()\n",
    "        for i, tf_dict in enumerate(self.tfs):\n",
    "            for key in tf_dict.keys():\n",
    "                self.tfidfs[i][key] = tf_dict[key] * self.idfs[key]\n",
    "\n",
    "    def query(self, sentence):\n",
    "        word_list = sentence.lower().split(' ')\n",
    "        word_gram_list = list(self._cal_two_gram(sentence))\n",
    "        word_list += word_gram_list\n",
    "        word_gram_list = list(self._cal_three_gram(sentence))\n",
    "        word_list += word_gram_list\n",
    "        length = len(word_list)\n",
    "        word_tf = [1 / length**0.5] * length\n",
    "        tf_query = dict(zip(word_list, word_tf))\n",
    "        tfidf_query = {w: tf_query[w] * self.idfs.get(w, 0) for w in word_list}\n",
    "\n",
    "        similarity_scores = []\n",
    "        for i, sentence in enumerate(self.tfidfs):\n",
    "            score = self.similarity(sentence, tfidf_query)\n",
    "            similarity_scores.append((score, self.train[i]))\n",
    "        return similarity_scores\n",
    "\n",
    "    def query_top(self, sentence):\n",
    "        return np.argsort(self.query(sentence))\n",
    "\n",
    "    def similarity(self, tfidf1, tfidf2):\n",
    "        dot_value = 0\n",
    "        for word in tfidf2:\n",
    "            if word not in tfidf1:\n",
    "                continue\n",
    "            dot_value += tfidf1[word] * tfidf2[word]\n",
    "        l1 = sum([i**2 for i in tfidf1.values()])**0.5\n",
    "        l2 = sum([i**2 for i in tfidf2.values()])**0.5\n",
    "        return dot_value / (l1 * l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
